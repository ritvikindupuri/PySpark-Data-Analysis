{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be02a957-7133-4d02-818e-fedeb3cecb05",
   "metadata": {},
   "source": [
    "# Project 13 -- Ritvik Indupuri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1228853-dd19-4ab2-89e0-0394d7d72de3",
   "metadata": {},
   "source": [
    "**TA Help:** John Smith, Alice Jones\n",
    "\n",
    "- Help with figuring out how to write a function.\n",
    "    \n",
    "**Collaboration:** Friend1, Friend2\n",
    "    \n",
    "- Helped figuring out how to load the dataset.\n",
    "- Helped debug error with my plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180e742-8e39-4698-98ff-5b00c8cf8ea0",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49445606-d363-41b4-b479-e319a9a84c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "# 1.1: Initialize a SparkSession and load the data\n",
    "spark = SparkSession.builder.appName(\"TDM_S\").config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"/anvil/projects/tdm/data/whin/weather.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112f0359-bd87-4079-8934-fe946e73af7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "|station_id|latitude|longitude|           name|    observation_time|temperature|temperature_high|temperature_low|humidity|solar_radiation|solar_radiation_high|rain|rain_inches_last_hour|wind_speed_mph|wind_direction_degrees|wind_gust_speed_mph|wind_gust_direction_degrees|pressure|soil_temp_1|soil_temp_2|soil_temp_3|soil_temp_4|soil_moist_1|soil_moist_2|soil_moist_3|soil_moist_4|\n",
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-10T04:00:00Z|       70.0|            71.0|           70.0|    83.0|           NULL|                NULL| 0.0|                  0.0|           0.0|                  NULL|                3.0|                      247.5|   30.05|       77.0|       78.0|       76.0|       74.0|        24.0|        24.0|        10.0|         9.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-10T04:15:00Z|       69.0|            70.0|           69.0|    84.0|           NULL|                NULL| 0.0|                  0.0|           1.0|                 247.5|                3.0|                      247.5|   30.04|       76.0|       78.0|       76.0|       74.0|        24.0|        25.0|        10.0|         9.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:00:00Z|       76.0|            77.0|           76.0|    76.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 202.5|                4.0|                      202.5|   29.89|       80.0|       80.0|       78.0|       75.0|        31.0|        30.0|        12.0|        10.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:15:00Z|       76.0|            76.0|           76.0|    77.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 202.5|                4.0|                      202.5|   29.88|       80.0|       80.0|       78.0|       75.0|        31.0|        31.0|        12.0|        10.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:30:00Z|       76.0|            76.0|           76.0|    77.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 225.0|                4.0|                      202.5|   29.88|       80.0|       80.0|       78.0|       75.0|        32.0|        31.0|        12.0|        10.0|\n",
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Show the first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456e57c-4a12-464b-999a-ef2df5af80c1",
   "metadata": {},
   "source": [
    "The process began by creating a SparkSession, which serves as the main entry point for working with PySpark. I set it up using the builder pattern (SparkSession.builder...getOrCreate()), and configured the session to allocate 2GB of driver memory through the .config() method.\n",
    "\n",
    "Once the session was ready, I loaded the dataset with spark.read.parquet(). At this stage, the resulting DataFrame (df) is a distributed structure—it doesn’t immediately pull all the data into memory like Pandas would. Instead, it represents a plan for how the data will be accessed and processed. \n",
    "\n",
    "The first actual computation happens when calling .show(5), since that command triggers Spark to execute the necessary operations and return the first five rows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc601975-35ed-4680-a4e1-0273ee3cc047",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16336a1-1ef0-41e8-bc7c-49387db27497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- observation_time: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- temperature_high: double (nullable = true)\n",
      " |-- temperature_low: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- solar_radiation: double (nullable = true)\n",
      " |-- solar_radiation_high: double (nullable = true)\n",
      " |-- rain: double (nullable = true)\n",
      " |-- rain_inches_last_hour: double (nullable = true)\n",
      " |-- wind_speed_mph: double (nullable = true)\n",
      " |-- wind_direction_degrees: double (nullable = true)\n",
      " |-- wind_gust_speed_mph: double (nullable = true)\n",
      " |-- wind_gust_direction_degrees: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- soil_temp_1: double (nullable = true)\n",
      " |-- soil_temp_2: double (nullable = true)\n",
      " |-- soil_temp_3: double (nullable = true)\n",
      " |-- soil_temp_4: double (nullable = true)\n",
      " |-- soil_moist_1: double (nullable = true)\n",
      " |-- soil_moist_2: double (nullable = true)\n",
      " |-- soil_moist_3: double (nullable = true)\n",
      " |-- soil_moist_4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deliverable 2.1\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64763804-8931-482c-a92c-8a9fe6b95f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to define operations: 0.1461491584777832 seconds\n"
     ]
    }
   ],
   "source": [
    "# Deliverable 2.2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the transformation\n",
    "result_df = df.groupBy(\"name\") \\\n",
    "              .agg(avg(\"humidity\").alias(\"average_humidity\"),\n",
    "                   avg(\"solar_radiation\").alias(\"average_solar_radiation\"),\n",
    "                   avg(\"pressure\").alias(\"average_pressure\")) \\\n",
    "              .filter((col(\"average_humidity\") > 50) & (col(\"average_solar_radiation\") > 100)) \\\n",
    "              .sort(desc(\"average_pressure\"))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to define operations: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1593747e-6056-41a8-8ddf-c02b443af0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----------------------+------------------+\n",
      "|            name| average_humidity|average_solar_radiation|  average_pressure|\n",
      "+----------------+-----------------+-----------------------+------------------+\n",
      "|WHIN089E-BENT009|74.77029578351164|     257.34197608558844|30.216278791692908|\n",
      "|      Bringhurst|80.17502311929556|     168.08889779559118|30.190432274525733|\n",
      "| WHIN020-FOUN001|76.72776597425857|     146.12622262805766|30.159863727349002|\n",
      "|        Idaville|77.37444567283207|     166.06921923967843|30.151303970200168|\n",
      "|         Wolcott|67.00495091507085|     104.93433155080214| 30.11409279198035|\n",
      "+----------------+-----------------+-----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "Time taken to show first 5 rows: 1.8336148262023926 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Deliverable 2.3\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# triggers the actual computation\n",
    "result_df.show(5)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to show first 5 rows: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc22d4-ddc3-41cc-a91a-cb0025bc0c80",
   "metadata": {},
   "source": [
    "This question shows Spark’s lazy evaluation model. In Deliverable 2.2, when the result_df was defined, the operation completed almost instantly because Spark didn’t actually run the query. Instead, it constructed a Directed Acyclic Graph (DAG) that described the sequence of tasks to be performed later.\n",
    "\n",
    "The actual computation occurred in step 2.3 with the .show(5) command. That action forced Spark to execute the plan, triggering the Catalyst optimizer to refine the instructions and then carry them out on the available resources (in this case, the local machine). Only at that point were the first five rows produced, which explains why the second timing measurement reflected a noticeably longer processing duration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586edd-ff26-4ce2-8f6b-2424b26f2929",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe0f40d-9655-4653-9ca8-886bdb61cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 3.1\n",
    "\n",
    "df.createOrReplaceTempView(\"weather_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a4749e8-827a-4941-8452-5ad389391c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to define SQL query: 0.11014533042907715 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Deliverable 3.2\n",
    "# Define the SQL query \n",
    "result_df_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        AVG(humidity) AS average_humidity,\n",
    "        AVG(solar_radiation) AS average_solar_radiation,\n",
    "        AVG(pressure) AS average_pressure\n",
    "    FROM weather_view\n",
    "    GROUP BY name\n",
    "    HAVING average_humidity > 50 AND average_solar_radiation > 100\n",
    "    ORDER BY average_pressure DESC\n",
    "\"\"\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to define SQL query: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e29c57d-1473-4a4f-b971-a5c3f8b2c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----------------------+------------------+\n",
      "|            name| average_humidity|average_solar_radiation|  average_pressure|\n",
      "+----------------+-----------------+-----------------------+------------------+\n",
      "|WHIN089E-BENT009|74.77029578351164|     257.34197608558844|30.216278791692908|\n",
      "|      Bringhurst|80.17502311929556|     168.08889779559118|30.190432274525733|\n",
      "| WHIN020-FOUN001|76.72776597425857|     146.12622262805766|30.159863727349002|\n",
      "|        Idaville|77.37444567283207|     166.06921923967843|30.151303970200168|\n",
      "|         Wolcott|67.00495091507085|     104.93433155080214| 30.11409279198035|\n",
      "+----------------+-----------------+-----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "Time taken to show first 5 rows (SQL): 0.840869665145874 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Deliverable 3.3\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# This 'action' triggers the computation\n",
    "result_df_sql.show(5)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to show first 5 rows (SQL): {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15e3a3-4ad0-4a57-b0d2-718255ad216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deliverable 3.4\n",
    "\n",
    "#Printing just the first few rows happens very quickly. On most systems it completes within a handful of seconds, often between one and five, though the exact time depends on how busy the machine is at that moment. The show() function is what actually kicks off the computation, since Spark has to run through the full set of operations—loading, grouping, filtering, and sorting—before it can display the sample output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fccd22-f675-4576-ab47-a4caac1ac077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 3.5\n",
    "\n",
    "#In practice, the performance is nearly identical. Whether you write the query with the DataFrame API or with Spark SQL, Spark translates both into the same optimized execution plan through its Catalyst engine. That means the underlying work being done is the same, so you shouldn’t expect any noticeable difference in how long it takes to show the first five rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6229f-35f7-400c-8366-c442baa5cf47",
   "metadata": {},
   "source": [
    "This question demonstrated that Spark SQL is fully integrated within PySpark. By creating a temporary view using createOrReplaceTempView, I was able to run a standard SQL query that replicated the same logic from Question 2. The key takeaway from Deliverable 3.5 was that the runtime was essentially identical to the DataFrame API approach. This confirms that Spark’s strength lies not in one interface over the other, but in the Catalyst optimizer, which translates both SQL queries and DataFrame operations into the same optimized execution plan. As a result, developers can choose whichever syntax they prefer without sacrificing performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f29c-d245-4d2b-9fc1-ca14cb6087d9",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cffc767-d1c8-4d64-b7dc-f0d2ee8a80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 4.1\n",
    "newdf = df.withColumn(\"observation_time\", to_timestamp(\"observation_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851d230c-232e-4e49-9a9b-93d1d05f8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deliverable 4.2\n",
    "\n",
    "newdf = newdf.withColumn(\"year\", year(\"observation_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86cbf4e-3d4f-479a-96c4-305e296c1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 4.3\n",
    "\n",
    "newdf = newdf.withColumn(\"month\", month(\"observation_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a7ad00a-21e2-4cb2-bac0-cf8b74639db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Deliverable 4.4: Group by month and aggregate\n",
    "result_monthly_df = newdf.groupBy(\"month\") \\\n",
    "                         .agg(avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                              avg(\"solar_radiation\").alias(\"avg_solar_radiation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70b5cb0-d656-471f-b66c-ad197920211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 4.5\n",
    "\n",
    "# Sort the results\n",
    "result_monthly_sorted = result_monthly_df.sort(desc(\"avg_temperature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ae387c2-3eb5-4c6a-81a7-7238516d4ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-------------------+\n",
      "|month|  avg_temperature|avg_solar_radiation|\n",
      "+-----+-----------------+-------------------+\n",
      "|    7| 73.9766158271431| 253.92342957199992|\n",
      "|    6|73.41862618893836| 267.88055625964927|\n",
      "|    8|71.00368368101219| 253.58956703962878|\n",
      "|    9|66.70186356073211|  179.6384488924556|\n",
      "|    5|60.18404719420269|  232.1310546777555|\n",
      "+-----+-----------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Deliverable 4.6: Print the top 5 results\n",
    "result_monthly_sorted.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d552245-b4d6-474a-9cc9-fa7b8e674d55",
   "metadata": {},
   "source": [
    "A key concept in this question was that Spark DataFrames are immutable. Unlike Pandas, you can’t directly alter a column in place. Instead, you use withColumn() to generate a new DataFrame that includes the updated or additional column. For the transformations, I applied functions such as to_timestamp, year, and month from pyspark.sql.functions to engineer the time‑based features. The final step was an aggregation using the familiar groupBy and agg pattern from Question 2, this time grouped on the newly created month column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9cdac-3e92-498f-83fa-e089bfc44ac8",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d370d7c9-06db-42b9-b75f-240481a5c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Sort [avg_temperature#651 DESC NULLS LAST], true, Statistics(sizeInBytes=9.9 MiB)\n",
      "+- Aggregate [month#619], [month#619, avg(temperature#481) AS avg_temperature#651, avg(solar_radiation#485) AS avg_solar_radiation#652], Statistics(sizeInBytes=9.9 MiB)\n",
      "   +- Project [temperature#481, solar_radiation#485, month(cast(cast(observation_time#480 as timestamp) as date)) AS month#619], Statistics(sizeInBytes=9.9 MiB)\n",
      "      +- Relation [station_id#476L,latitude#477,longitude#478,name#479,observation_time#480,temperature#481,temperature_high#482,temperature_low#483,humidity#484,solar_radiation#485,solar_radiation_high#486,rain#487,rain_inches_last_hour#488,wind_speed_mph#489,wind_direction_degrees#490,wind_gust_speed_mph#491,wind_gust_direction_degrees#492,pressure#493,soil_temp_1#494,soil_temp_2#495,soil_temp_3#496,soil_temp_4#497,soil_moist_1#498,soil_moist_2#499,soil_moist_3#500,... 1 more fields] parquet, Statistics(sizeInBytes=85.1 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_temperature#651 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_temperature#651 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "      +- HashAggregate(keys=[month#619], functions=[avg(temperature#481), avg(solar_radiation#485)], output=[month#619, avg_temperature#651, avg_solar_radiation#652])\n",
      "         +- Exchange hashpartitioning(month#619, 200), ENSURE_REQUIREMENTS, [plan_id=273]\n",
      "            +- HashAggregate(keys=[month#619], functions=[partial_avg(temperature#481), partial_avg(solar_radiation#485)], output=[month#619, sum#689, count#690L, sum#691, count#692L])\n",
      "               +- Project [temperature#481, solar_radiation#485, month(cast(cast(observation_time#480 as timestamp) as date)) AS month#619]\n",
      "                  +- FileScan parquet [observation_time#480,temperature#481,solar_radiation#485] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/anvil/projects/tdm/data/whin/weather.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<observation_time:string,temperature:double,solar_radiation:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deliverable 5.1\n",
    "result_monthly_sorted.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce43e22-fe9e-48b2-a507-85d1b2adae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 5.2\n",
    "\n",
    "#To determine the counts, you need to execute the code from Deliverable 5.1 and then review the output under the headings == Physical Plan == and == Optimized Logical Plan ==. The totals can differ slightly depending on the Spark release you’re working with.\n",
    "#- Physical Plan: Typically contains a larger number of operations, often in the range of 7–10. These are low‑level execution details such as HashAggregate, Exchange, and FileScan.\n",
    "#- Optimized Logical Plan: Usually shorter, around 3–5 steps. It represents the computation at a higher level, with abstract operations like Aggregate, Sort, and Project.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5261e38e-82ec-4dee-9e4c-9d82890f54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Deliverable 5.3\n",
    "\n",
    "# This query performs a self-join on the station dataset to identify temperature\n",
    "# measurements taken exactly one hour apart at the same station. It then computes\n",
    "# the one-hour temperature change, aggregates results by station, and filters for\n",
    "# \"busy\" stations (those with more than 1000 observations). Finally, it sorts the\n",
    "# stations by their average temperature change in descending order.\n",
    "\n",
    "\n",
    "df_with_time = df.withColumn(\"observation_time\", to_timestamp(\"observation_time\"))\n",
    "\n",
    "df_t1 = df_with_time.alias(\"t1\")\n",
    "df_t2 = df_with_time.alias(\"t2\")\n",
    "\n",
    "complex_query = (\n",
    "    df_t1.join(\n",
    "        df_t2,\n",
    "        (col(\"t1.station_id\") == col(\"t2.station_id\")) &\n",
    "        (col(\"t1.observation_time\") == expr(\"t2.observation_time - INTERVAL 1 HOUR\"))\n",
    "    )\n",
    "    .select(\n",
    "        col(\"t1.station_id\"),\n",
    "        col(\"t1.name\").alias(\"station_name\"),\n",
    "        (col(\"t2.temperature\") - col(\"t1.temperature\")).alias(\"temp_change_1hr\")\n",
    "    )\n",
    "    .groupBy(\"station_name\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_observations\"),\n",
    "        avg(\"temp_change_1hr\").alias(\"avg_temp_change\")\n",
    "    )\n",
    "    .filter(col(\"num_observations\") > 1000)\n",
    "    .sort(desc(\"avg_temp_change\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21d47ae4-48a2-469d-877e-af9f17a18e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Sort [avg_temp_change#836 DESC NULLS LAST], true, Statistics(sizeInBytes=121.3 TiB)\n",
      "+- Filter (num_observations#835L > 1000), Statistics(sizeInBytes=121.3 TiB)\n",
      "   +- Aggregate [station_name#833], [station_name#833, count(1) AS num_observations#835L, avg(temp_change_1hr#834) AS avg_temp_change#836], Statistics(sizeInBytes=121.3 TiB)\n",
      "      +- Project [name#479 AS station_name#833, (temperature#810 - temperature#481) AS temp_change_1hr#834], Statistics(sizeInBytes=99.2 TiB)\n",
      "         +- Join Inner, ((station_id#476L = station_id#805L) AND (observation_time#804 = observation_time#831 + INTERVAL '-01' HOUR)), Statistics(sizeInBytes=209.4 TiB)\n",
      "            :- Project [station_id#476L, name#479, cast(observation_time#480 as timestamp) AS observation_time#804, temperature#481], Statistics(sizeInBytes=18.4 MiB)\n",
      "            :  +- Filter (isnotnull(station_id#476L) AND isnotnull(cast(observation_time#480 as timestamp))), Statistics(sizeInBytes=85.1 MiB)\n",
      "            :     +- Relation [station_id#476L,latitude#477,longitude#478,name#479,observation_time#480,temperature#481,temperature_high#482,temperature_low#483,humidity#484,solar_radiation#485,solar_radiation_high#486,rain#487,rain_inches_last_hour#488,wind_speed_mph#489,wind_direction_degrees#490,wind_gust_speed_mph#491,wind_gust_direction_degrees#492,pressure#493,soil_temp_1#494,soil_temp_2#495,soil_temp_3#496,soil_temp_4#497,soil_moist_1#498,soil_moist_2#499,soil_moist_3#500,... 1 more fields] parquet, Statistics(sizeInBytes=85.1 MiB)\n",
      "            +- Project [station_id#805L, cast(observation_time#809 as timestamp) AS observation_time#831, temperature#810], Statistics(sizeInBytes=11.4 MiB)\n",
      "               +- Filter (isnotnull(station_id#805L) AND isnotnull(cast(observation_time#809 as timestamp))), Statistics(sizeInBytes=85.1 MiB)\n",
      "                  +- Relation [station_id#805L,latitude#806,longitude#807,name#808,observation_time#809,temperature#810,temperature_high#811,temperature_low#812,humidity#813,solar_radiation#814,solar_radiation_high#815,rain#816,rain_inches_last_hour#817,wind_speed_mph#818,wind_direction_degrees#819,wind_gust_speed_mph#820,wind_gust_direction_degrees#821,pressure#822,soil_temp_1#823,soil_temp_2#824,soil_temp_3#825,soil_temp_4#826,soil_moist_1#827,soil_moist_2#828,soil_moist_3#829,... 1 more fields] parquet, Statistics(sizeInBytes=85.1 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_temp_change#836 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_temp_change#836 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=330]\n",
      "      +- Filter (num_observations#835L > 1000)\n",
      "         +- HashAggregate(keys=[station_name#833], functions=[count(1), avg(temp_change_1hr#834)], output=[station_name#833, num_observations#835L, avg_temp_change#836])\n",
      "            +- Exchange hashpartitioning(station_name#833, 200), ENSURE_REQUIREMENTS, [plan_id=326]\n",
      "               +- HashAggregate(keys=[station_name#833], functions=[partial_count(1), partial_avg(temp_change_1hr#834)], output=[station_name#833, count#845L, sum#846, count#847L])\n",
      "                  +- Project [name#479 AS station_name#833, (temperature#810 - temperature#481) AS temp_change_1hr#834]\n",
      "                     +- SortMergeJoin [station_id#476L, observation_time#804], [station_id#805L, observation_time#831 + INTERVAL '-01' HOUR], Inner\n",
      "                        :- Sort [station_id#476L ASC NULLS FIRST, observation_time#804 ASC NULLS FIRST], false, 0\n",
      "                        :  +- Exchange hashpartitioning(station_id#476L, observation_time#804, 200), ENSURE_REQUIREMENTS, [plan_id=318]\n",
      "                        :     +- Project [station_id#476L, name#479, cast(observation_time#480 as timestamp) AS observation_time#804, temperature#481]\n",
      "                        :        +- Filter (isnotnull(station_id#476L) AND isnotnull(cast(observation_time#480 as timestamp)))\n",
      "                        :           +- FileScan parquet [station_id#476L,name#479,observation_time#480,temperature#481] Batched: true, DataFilters: [isnotnull(station_id#476L), isnotnull(cast(observation_time#480 as timestamp))], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/anvil/projects/tdm/data/whin/weather.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct<station_id:bigint,name:string,observation_time:string,temperature:double>\n",
      "                        +- Sort [station_id#805L ASC NULLS FIRST, observation_time#831 + INTERVAL '-01' HOUR ASC NULLS FIRST], false, 0\n",
      "                           +- Exchange hashpartitioning(station_id#805L, observation_time#831 + INTERVAL '-01' HOUR, 200), ENSURE_REQUIREMENTS, [plan_id=319]\n",
      "                              +- Project [station_id#805L, cast(observation_time#809 as timestamp) AS observation_time#831, temperature#810]\n",
      "                                 +- Filter (isnotnull(station_id#805L) AND isnotnull(cast(observation_time#809 as timestamp)))\n",
      "                                    +- FileScan parquet [station_id#805L,observation_time#809,temperature#810] Batched: true, DataFilters: [isnotnull(station_id#805L), isnotnull(cast(observation_time#809 as timestamp))], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/anvil/projects/tdm/data/whin/weather.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct<station_id:bigint,observation_time:string,temperature:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deliverable 5.4\n",
    "\n",
    "complex_query.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afc508-7f34-4cac-ad31-82a2843c0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 5.5\n",
    "\n",
    "#For the complex query I ran, the Physical Plan contained 18 steps, while the Optimized Logical Plan had 7 steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab272d-bf22-4161-9089-ec3102d75a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 5.6\n",
    "\n",
    "#The Optimized Logical Plan was far easier to interpret. It laid out the high‑level operations—such as Sort, Filter, Aggregate, and Join—almost like a recipe that directly reflected the query I wrote. \n",
    "#In contrast, the Physical Plan was much more detailed, showing the intricate details of execution with operations like SortMergeJoin, HashAggregate, and Exchange (which handles data shuffling). The Physical Plan is essential for performance tuning, but the Logical Plan is what makes the query’s intent clear to a human reader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf00fb-2418-460f-ae94-2a32b0c28952",
   "metadata": {},
   "source": [
    "The .explain() method is invaluable for both debugging and optimization because it exposes two complementary perspectives. The Optimized Logical Plan provides a concise, human‑readable outline of the query logic (joins, filters, aggregations, etc.), while the Physical Plan dives into the low‑level execution details, including how Spark performs joins, aggregates, and data shuffles. As seen in my complex query from 5.3, the Physical Plan can grow lengthy and intricate, whereas the Logical Plan remains compact and focused on the business logic. This makes the Logical Plan the better tool when the goal is simply to understand what the query is doing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76442d6-d02e-4f26-b9d6-c3183e1d6929",
   "metadata": {},
   "source": [
    "## Pledge\n",
    "\n",
    "By submitting this work I hereby pledge that this is my own, personal work. I've acknowledged in the designated place at the top of this file all sources that I used to complete said work, including but not limited to: online resources, books, and electronic communications. I've noted all collaboration with fellow students and/or TA's. I did not copy or plagiarize another's work.\n",
    "\n",
    "> As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do. Accountable together – We are Purdue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
